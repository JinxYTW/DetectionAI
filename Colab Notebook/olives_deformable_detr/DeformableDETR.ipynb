{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import des libraries"
      ],
      "metadata": {
        "id": "4UgJNT25dZMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 torchvision==0.15.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZly1ewYiRpi",
        "outputId": "d6b64e09-cfcb-4143-8697-f2946d74151a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5.1 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.5.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/Deformable-DETR', ignore_errors=True)"
      ],
      "metadata": {
        "id": "Z-DsdeVLiUKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fundamentalvision/Deformable-DETR.git\n",
        "%cd /content/Deformable-DETR\n",
        "!pip install -q -r requirements.txt --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiWxh-xTfPBV",
        "outputId": "3631d192-6c9f-47c8-eeb5-cfc9305480d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Deformable-DETR'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 98 (delta 27), reused 25 (delta 25), pack-reused 37 (from 1)\u001b[K\n",
            "Receiving objects: 100% (98/98), 383.50 KiB | 3.65 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "/content/Deformable-DETR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Deformable-DETR')"
      ],
      "metadata": {
        "id": "3tk3hoEjhEuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trw-xMK-deSG",
        "outputId": "69abaae9-f912-4945-e667-3ca4903fca58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation"
      ],
      "metadata": {
        "id": "NvVn41kTeUnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms as T\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision.datasets import CocoDetection\n",
        "from util.misc import nested_tensor_from_tensor_list\n",
        "from models.deformable_detr import build_deformable_detr\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y_NNZH9neWdu",
        "outputId": "452c5438-47c5-486a-a272-4ce49999c181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-8-1bc961904702>\", line 2, in <cell line: 0>\n",
            "    import torchvision.transforms as T\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import datasets, io, models, ops, transforms, utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
            "    from .anchor_utils import AnchorGenerator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
            "    class AnchorGenerator(nn.Module):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
            "    device: torch.device = torch.device(\"cpu\"),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_NewEmptyTensorOp' from 'torchvision.ops.misc' (/usr/local/lib/python3.11/dist-packages/torchvision/ops/misc.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1bc961904702>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnested_tensor_from_tensor_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_deformable_detr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Deformable-DETR/util/misc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_NewEmptyTensorOp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_size_scale_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# type: (int, Optional[List[int]], Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_NewEmptyTensorOp' from 'torchvision.ops.misc' (/usr/local/lib/python3.11/dist-packages/torchvision/ops/misc.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation du dataset"
      ],
      "metadata": {
        "id": "Fa4f6YjzeZTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip -o coco128.zip\n",
        "!unzip -q coco128.zip"
      ],
      "metadata": {
        "id": "PTjjmwcSfUX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# Dossiers d'entrée\n",
        "images_dir = Path(\"/content/coco128/images/train2017\")\n",
        "labels_dir = Path(\"/content/coco128/labels/train2017\")\n",
        "\n",
        "# Dictionnaire COCO de sortie\n",
        "coco_dict = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Mapping YOLO class_id → label name\n",
        "# Tu peux adapter cela avec les vrais noms si tu les as\n",
        "coco_class_names = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\",\n",
        "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
        "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
        "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
        "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
        "    \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\",\n",
        "    \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\",\n",
        "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
        "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\",\n",
        "    \"toothbrush\"\n",
        "]\n",
        "  # À adapter\n",
        "for idx, name in enumerate(coco_class_names):\n",
        "    coco_dict[\"categories\"].append({\n",
        "        \"id\": idx,\n",
        "        \"name\": name,\n",
        "        \"supercategory\": \"object\"\n",
        "    })\n",
        "\n",
        "annotation_id = 1  # ID global pour chaque annotation\n",
        "\n",
        "for image_id, image_path in enumerate(sorted(images_dir.glob(\"*.jpg\"))):\n",
        "    # Charger l'image pour obtenir ses dimensions\n",
        "    img = Image.open(image_path)\n",
        "    width, height = img.size\n",
        "\n",
        "    # Ajouter info image\n",
        "    coco_dict[\"images\"].append({\n",
        "        \"id\": image_id,\n",
        "        \"file_name\": image_path.name,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    })\n",
        "\n",
        "    # Correspondant .txt\n",
        "    label_path = labels_dir / f\"{image_path.stem}.txt\"\n",
        "    if not label_path.exists():\n",
        "        continue\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            class_id, x_center, y_center, w, h = map(float, parts)\n",
        "            class_id = int(class_id)\n",
        "\n",
        "            # Convertir en format COCO (bbox = [x_min, y_min, width, height])\n",
        "            x_min = (x_center - w / 2) * width\n",
        "            y_min = (y_center - h / 2) * height\n",
        "            bbox_width = w * width\n",
        "            bbox_height = h * height\n",
        "\n",
        "            coco_dict[\"annotations\"].append({\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": class_id,\n",
        "                \"bbox\": [x_min, y_min, bbox_width, bbox_height],\n",
        "                \"area\": bbox_width * bbox_height,\n",
        "                \"iscrowd\": 0\n",
        "            })\n",
        "            annotation_id += 1\n",
        "\n",
        "# Sauvegarder le fichier COCO\n",
        "output_path = \"/content/coco128/annotations/instances_train2017.json\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(coco_dict, f, indent=2)\n",
        "\n",
        "print(f\"Annotations COCO sauvegardées dans {output_path}\")"
      ],
      "metadata": {
        "id": "W-8SdK10gU1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoWrapper(torch.utils.data.Dataset):\n",
        "    def __init__(self, coco_dataset):\n",
        "        self.coco_dataset = coco_dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, targets = self.coco_dataset[idx]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for target in targets:\n",
        "            if 'bbox' in target:\n",
        "                x, y, w, h = target['bbox']\n",
        "                boxes.append([x, y, x + w, y + h])\n",
        "                labels.append(target['category_id'])\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
        "            labels = torch.empty((0,), dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target_out = {'boxes': boxes, 'labels': labels}\n",
        "        return img, target_out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coco_dataset)\n"
      ],
      "metadata": {
        "id": "4fR5Jzeted83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([T.ToTensor()])\n",
        "coco_dataset = CocoDetection(\n",
        "    root='/content/coco128/images/train2017',\n",
        "    annFile='/content/coco128/annotations/instances_train2017.json',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "wrapped_dataset = CocoWrapper(coco_dataset)\n",
        "indices = torch.randperm(len(wrapped_dataset)).tolist()\n",
        "train_dataset = Subset(wrapped_dataset, indices[:-50])\n",
        "valid_dataset = Subset(wrapped_dataset, indices[-50:])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "BiJWv0fteha1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mise en place du modèle"
      ],
      "metadata": {
        "id": "ETVUeolnel8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# Création du modèle\n",
        "model, criterion, postprocessors = build_deformable_detr(num_classes=91,device=device)  # COCO has 91 classes\n",
        "\n",
        "# Envoi du modèle sur le device\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Optimiseur\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "DamHluu6eoTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "mNL1lZn0etpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_dicts = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad]\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "        \"lr\": 1e-5,\n",
        "    },\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.AdamW(param_dicts, lr=1e-4, weight_decay=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "LYpZnI5mgA-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    train_one_epoch(model, criterion, train_loader, optimizer, device, epoch, print_freq=100)\n",
        "    lr_scheduler.step()\n",
        "    evaluate(model, criterion, postprocessors, val_loader, device=device)\n",
        "\n",
        "print(\"\\n✅ Entraînement terminé\")"
      ],
      "metadata": {
        "id": "yBQCY-7SgFVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(2):\n",
        "    running_loss = 0.0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [\n",
        "            {k: v.to(device) for k, v in t.items()} for t in targets\n",
        "        ]\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += losses.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"\\n✅ Entraînement terminé\")"
      ],
      "metadata": {
        "id": "GMAEZVPXev0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}